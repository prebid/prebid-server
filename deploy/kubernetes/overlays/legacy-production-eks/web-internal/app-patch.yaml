---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tpe-prebid-service
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::266171351246:role/tpe_prebid_service
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tpe-prebid-service
spec:
  template:
    spec:
      volumes:
        - name: tapjoy-tooling
          configMap:
            name: tpe-prebid-service-tooling
            defaultMode: 0755 # Make scripts executable
      containers:
        - name: app
          command: ["/tapjoy/tooling/entrypoint-aws.sh"]
          args:
            # TODO Remove env var overrides once app runs exclusively in k8s or the env vars are no longer published
            # PBS_ADAPTERS_TAPJOY_ENDPOINT setup is a gift for devs living in a future when optsoa runs in EKS
            - chamber exec devops/aws/production/tpe_prebid_service-web_internal/env --
              env PBS_MONITORING_OPENTELEMETRY_ENDPOINT=$$(printenv MY_NODE_IP):4139
              ./tpe_prebid_service
          # App is very high throughput (~30k QPS), which results in high pod count & churn at lower CPU requests, which
          # makes monitoring in NR/SFX difficult due to timeseries cardinality. Make pods relatively large to allow
          # each instance to do more work.
          resources:
            requests:
              # 2022/05/10: 500m CPU request was observed to result in ~2.4k RPM (40 QPS) per pod, w/ ~667 pods. 10x
              # should result in ~24k RPM (400 QPS) and ~67 pods.
              cpu: "5"
              memory: 150Mi # 2022/05/10: EC2 at full throughput shows ~50Mi usage in New Relic
            # 2022/05/10: Leave plenty of overhead, and we can tighten it up later
            limits:
              cpu: "20"
              memory: 2000Mi
          volumeMounts:
            - name: tapjoy-tooling
              mountPath: /tapjoy/tooling
              readOnly: true
          # Attempt to restart container if the app is truly unavailable for 1min
          livenessProbe:
            tcpSocket:
              port: app
            failureThreshold: 6
            successThreshold: 1
          # Introduce a delay to the shutdown sequence to wait for the
          # pod to be taken out of Service backends.
          ## https://blog.gruntwork.io/delaying-shutdown-to-wait-for-pod-deletion-propagation-445f779a8304
          lifecycle:
            preStop:
              exec:
                command: ["sleep", "30"]
          # Introduce a delay to the shutdown sequence to wait for the
          # pod to be taken out of Service backends.
          ## https://blog.gruntwork.io/delaying-shutdown-to-wait-for-pod-deletion-propagation-445f779a8304
        - name: nginx
          readinessProbe:
            periodSeconds: 1
            failureThreshold: 1
            successThreshold: 3
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "sleep 30 && /usr/sbin/nginx -s quit"]
